---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Adaptive Prior-Dependent Correction Enhanced Reinforcement Learning for Natural Language Generation
subtitle: ''
summary: ''
authors:
- Wei Cheng
- Ziyan Luo
- Qiyue Yin
tags: [Natural Language Generation, Reinforcement Learning, Adaptive Prior-Dependent Correction]
categories: [NLP, RL]
date: '2021-01-01'
lastmod: 2020-10-13T15:37:32+08:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2020-10-13T07:37:31.831377Z'
publication_types:
- '1'
abstract: "Natural language generation (NLG) is an important task with various applications\
  \ like neural machine translation (NMT) and image captioning. Since deep-learning-based\
  \ methods have issues of exposure bias and loss inconsistency, reinforcement learning\
  \ (RL) is widely adopted in NLG tasks recently. But most RL-based methods ignore\
  \ the deviation ignorance issue, which means the model fails to understand the extent\
  \ of token-level deviation well. It leads to semantic incorrectness and hampers\
  \ the agent to perform well. To address the issue, we propose a technique called\
  \ adaptive prior-dependent correction (APDC) to enhance RL. It leverages the distribution\
  \ generated by computing the distances between the ground truth and all other words\
  \ to correct the agent's stochastic policy. Additionally, some techniques on RL\
  \ are explored to coordinate RL with APDC, which requires a reward estimation at\
  \ every time step. We find that the RL-based NLG tasks are a special case in RL,\
  \ where the state transition is deterministic and the afterstate value equals the\
  \ Q-value at every time step. To utilize such prior knowledge, we estimate the advantage\
  \ function with the difference of the Q-values which can be estimated by Monte Carlo\
  \ rollouts. Experiments show that, on three tasks of NLG (NMT, image captioning,\
  \ abstractive text summarization), our method consistently outperforms the state-of-the-art\
  \ RL-based approaches on different frequently-used metrics. "
publication: 'AAAI-21 (accepted)'
url_pdf: files/APDC_AAAI21.pdf
url_slides: files/apdc_15min.pptx
---
