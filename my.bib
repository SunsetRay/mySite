
@inproceedings{match_2021,
	title = {Match Plan Generation in Web Search with Parameterized Action Reinforcement Learning},
	author = {Luo, Ziyan, ...},
	abstract = {To achieve good result quality and short query response time, search engines use specific match plans on Inverted Index to help retrieve a small set of relevant documents from billions of web pages. A match plan is composed of a sequence of match rules, which contain discrete match rule types and continuous stopping quotas. Currently, match plans are manually designed by experts according to their several yearsâ€™ experience, which encounter difficulty in dealing with heterogeneous queries and varying data distribution. In this work, we formulate the match plan generation as a Partially Observable Markov Decision Process ({POMDP}) with a discrete-continuous hybrid action space, and propose a novel reinforcement learning algorithm Parameterized Action Soft Actor-Critic ({PASAC}) to effectively enhance the exploration in both spaces. We also introduce Stratified Prioritized Experience Replay ({SPER}) to address the challenge that the samples with high priority often center on a small range of the reward space. We are the first group to generalize this task for all queries as a learning problem with zero prior knowledge and successfully apply deep reinforcement learning in the real web search environment. Our approach greatly outperforms the well designed production match plans by over 70% reduction of index block accesses with the quality of documents almost unchanged, and 9% reduction of query response time even with model inference cost. Our method also beats the baselines on some open-source benchmarks.},
	pages = {10},
	date = {2021},
	langid = {english}
}

@inproceedings{adaptive_2021,
	title = {Adaptive Prior-Dependent Correction Enhanced Reinforcement Learning for Natural Language Generation},
	author = {Luo, Ziyan, ...},
	abstract = {Natural language generation (NLG) is an important task with various applications like neural machine translation (NMT) and image captioning. Since deep-learning-based methods have issues of exposure bias and loss inconsistency, reinforcement learning (RL) is widely adopted in NLG tasks recently. But most RL-based methods ignore the deviation ignorance issue, which means the model fails to understand the extent of token-level deviation well. It leads to semantic incorrectness and hampers the agent to perform well. To address the issue, we propose a technique called adaptive prior-dependent correction (APDC) to enhance RL. It leverages the distribution generated by computing the distances between the ground truth and all other words to correct the agent's stochastic policy. Additionally, some techniques on RL are explored to coordinate RL with APDC, which requires a reward estimation at every time step. We find that the RL-based NLG tasks are a special case in RL, where the state transition is deterministic and the afterstate value equals the Q-value at every time step. To utilize such prior knowledge, we estimate the advantage function with the difference of the Q-values which can be estimated by Monte Carlo rollouts. Experiments show that, on three tasks of NLG (NMT, image captioning, abstractive text summarization), our method consistently outperforms the state-of-the-art RL-based approaches on different frequently-used metrics. },
	pages = {10},
	date = {2021},
	langid = {english}
}

@inproceedings{rlmob_2021,
	title = {Human Mobility Prediction, Successive Mobility Prediction, Deep Reinforcement Learning, Sequential Data Mining},
	author = {Luo, Ziyan and Miao, Congcong},
	abstract = {Human mobility prediction is an important task in the field of spatiotemporal sequential data mining and urban computing. Despite the extensive work on mining human mobility behavior, little attention was paid to the problem of successive mobility prediction. At present, the state-of-the-art methods of human mobility prediction are mainly based on traditional machine learning and deep learning. 
To achieve higher predictability and adapt well to the successive mobility prediction, there are four key challenges:
1) disability to the circumstance that the optimizing target is discrete-continuous hybrid and non-differentiable. In our work, we assume that  the user's demands are always multi-targeted and can be modeled as a discrete-continuous hybrid function;
2) difficulty to alter the recommendation strategy flexibly according to the changes in user needs in real scenarios; 
3) error propagation and exposure bias issues when predicting multiple points in successive mobility prediction;
4) cannot interactively explore user's potential interest that does not appear in the history.
While previous methods met these difficulties, reinforcement learning (RL) is an intuitive answer for this task to settle these issues.
To the best of our knowledge, this work is the first one to introduce RL to this task.
In this paper, we formulate this problem as a Markov Decision Process.
We further propose a framework - RLMob to solve our problem. A simulated environment is carefully designed. An actor-critic framework with an instance of Proximal Policy Optimization (PPO) is applied to adapt to our scene with a large state space.
Experiments show that on the task, the performance of our approach 
is consistently superior to that of the deep-learning-based approaches. },
	pages = {10},
	date = {2021},
	langid = {english}
}

@inproceedings{miao_predicting_2020,
	location = {Houston {TX} {USA}},
	title = {Predicting Human Mobility via Attentive Convolutional Network},
	isbn = {978-1-4503-6822-3},
	url = {https://dl.acm.org/doi/10.1145/3336191.3371846},
	doi = {10.1145/3336191.3371846},
	abstract = {Predicting human mobility is an important trajectory mining task for various applications, ranging from smart city planning to personalized recommendation system. While most of previous works adopt {GPS} tracking data to model human mobility, the recent fastgrowing geo-tagged social media ({GTSM}) data brings new opportunities to this task. However, predicting human mobility on {GTSM} data is not trivial because of three challenges: 1) extreme data sparsity; 2) high order sequential patterns of human mobility and 3) evolving preference of users for tagging.},
	eventtitle = {{WSDM} '20: The Thirteenth {ACM} International Conference on Web Search and Data Mining},
	pages = {438--446},
	booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
	publisher = {{ACM}},
	author = {Miao, Congcong and Luo, Ziyan and Zeng, Fengzhu and Wang, Jilong},
	urldate = {2020-05-22},
	date = {2020-01-20},
	langid = {english}
}

